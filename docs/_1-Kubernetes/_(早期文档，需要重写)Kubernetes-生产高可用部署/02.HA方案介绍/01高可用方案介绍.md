---
title: 01.HA方案介绍
---



参考文档：https://github.com/kubernetes/kubeadm/blob/master/docs/ha-considerations.md#options-for-software-load-balancing

## 端口占用

**Note:** 搭建集群时脚本会关闭节点上防火墙，集群搭建完毕后会使用到如下表列出的端口：

| **离线安装时kubeadm-ha节点** |      |             |                              |                     |
| ---------------------------- | ---- | ----------- | ---------------------------- | ------------------- |
| TCP                          | 入站 | 5000        | registry                     | All                 |
| TCP                          | 入站 | 12480       | yum repository               | All                 |
| **控制平面节点**             |      |             |                              |                     |
| 协议                         | 方向 | 端口范围    | 使用者                       | 用途                |
| TCP                          | 入站 | 6443        | Kubernetes APIserver         | All                 |
| TCP                          | 入站 | 2379-2380   | etcd server clientAPI        | kube-apiserver,etcd |
| TCP                          | 入站 | 10248,10250 | Kubelet API                  | Self,Controlplane   |
| TCP                          | 入站 | 10251,10259 | kube-scheduler               | Self                |
| TCP                          | 入站 | 10252,10257 | kube-controller-manager      | Self                |
| **工作节点**                 |      |             |                              |                     |
| 协议                         | 方向 | 端口范围    | 使用者                       | 用途                |
| tcp                          | 入站 | 80          | ingress-controller           | All                 |
| tcp                          | 入站 | 443         | ingress-controller           | All                 |
| tcp                          | 入站 | 18443       | ingress-controller           | Self                |
| tcp                          | 入站 | 10254       | ingress-controller           | Self                |
| TCP                          | 入站 | 10248,10250 | KubeletAPI                   | Self,Controlplane   |
| TCP                          | 入站 | 30000-32767 | NodePort Services            | All                 |
| TCP                          | 入站 | 10256       | kube-proxy                   | 健康检查            |
| **Flannel**                  |      |             |                              |                     |
| 协议                         | 方向 | 端口范围    | 使用者                       | 用途                |
| UDP                          | 双向 | 8285        | flannel networking(UDP)      | 收发封装数据包      |
| UDP                          | 双向 | 8472        | flannel networking(VXLAN)    | 收发封装数据包      |
| **Calico**                   |      |             |                              |                     |
| 协议                         | 方向 | 端口范围    | 使用者                       | 用途                |
| TCP                          | 双向 | 179         | Calico networking(BGP)       | 收发封装数据包      |
| TCP                          | 双向 | 5473        | Calico networking with Typha | 收发封装数据包      |
| **load-balancer**            |      |             |                              |                     |
| 协议                         | 方向 | 端口范围    | 使用者                       | 用途                |
| tcp                          | 入站 | 112         | keepalived                   | lb kube-apiserver   |
| tcp                          | 入站 | 8443        | nginx,haproxy,envoy          | lb kube-apiserver   |
| tcp                          | 入站 | 8081        | nginx,haproxy,envoy          | 健康检查            |
| tcp                          | 入站 | 9090        | haproxy,envoy                | 管理端口            |

## Kube-apiserver负载模式

### 本地负载均衡

该负载均衡模式是在节点本地部署一个负载均衡器，节点本地所有需要链接 apiserver 的组件均通过本地负载均衡器进行访问。
- 优点：兼容所有云；无额外的网络消耗（共用主机 network namespace）；不会出现 lb 宕机而整个集群崩溃的情况。
- 缺点：集群外需要链接 apiserver 无法做到高可用（除非再搭建一套负载均衡）；节点本地负载均衡器宕机则该节点无法正常工作；添加或删除 master 节点会涉及所有节点更新负载均衡器配置（当然不更新也是可以的）。

```
+------------------------+                +------------------------+
|        master-A        |                |        master-B        |
+------------------------+                +------------------------+
|          nginx         +-------+-------->       apiserver        |
+-----^------+------^----+       |        +-----------^------------+
|     |      |      |    |       |        |           |            |
+-----+----+ | +----+----+       |        +---------+ | +----------+
|controller| | |scheduler|       |        |scheduler| | |controller|
+----------+ | +---------+       |        +----+----+ | +-----+----+
|            |           |       |        |    |      |       |    |
+------------v-----------+       |        +----v------+-------v----+
|        apiserver       <-------+--------+         nginx          |
+------------------------+       |        +------------------------+
                                 |
          >----------------------^----------------------<
          |                      |                      |
+---------+----------+ +---------+----------+ +---------+----------+
|       nginx        | |       nginx        | |       nginx        |
+---^-----------^----+ +---^-----------^----+ +---^-----------^----+
|   |           |    | |   |           |    | |   |           |    |
+---+---+ +-----+----+ +---+---+ +-----+----+ +---+---+ +-----+----+
|kubelet| |kube-proxy| |kubelet| |kube-proxy| |kubelet| |kube-proxy|
+-------+ +----------+ +-------+ +----------+ +-------+ +----------+
|                    | |                    | |                    |
+--------------------+ +--------------------+ +--------------------+
|       node-A       | |       node-B       | |       node-C       |
+--------------------+ +--------------------+ +--------------------+
```

### VIP负载均衡

该负载均衡模式是在集群外搭建一个主备负载，虚拟IP（VIP）飘在这些节点上，当节点挂掉虚拟IP（VIP）会迅速转移到正常工作的节点上，该模式常见的组合即为：HAproxy + keepalived。
- 优点：集群内外链接 apiserver 均为高可用。
- 缺点：公有云无法使用；额外的网络消耗；所有node的网络I/O都会高度集中于一台机器上（VIP)，一旦集群节点增多，pod增多，单机的网络I/O迟早是网络隐患；lb 宕机整个集群崩溃（当然这种情况很少）。

```
  +----------------------+                  +----------------------+
  |       master-A       |                  |        master-B      |
  +----------------------+                  +----------------------+
  |       apiserver      <---------+-------->        apiserver     |
  +----------------------+         |        +----------------------+
  |                      |         |        |                      |
  +----------+ +---------+         |        +---------+ +----------+
  |controller| |scheduler|         |        |scheduler| |controller|
  +----+-----------+-----+         |        +------+----------+----+
       |           |               |               |          |
v------v-----------v  >------------^------------<  v----------v------v
|                     |                         |                    |
| +-------------------+-------------------------+------------------+ |
| |keepalived|    |HAproxy|                 |HAprox |   |keepalived| |
| +----------+    +-------+                 +-------+   +----------+ |
| |                       |       VIP       |                      | |
| |         LB-A          |                 |          LB-B        | |
| +-----------------------+--------^--------+----------------------+ |
|                                  |                                 |
>---->----------->---------->------^----<----------<-----------<-----<
     |           |          |           |          |           |
 +---+-----------+----+ +---+-----------+----+ +---+-----------+----+
 |kubelet| |kube-proxy| |kubelet| |kube-proxy| |kubelet| |kube-proxy|
 +-------+ +----------+ +-------+ +----------+ +-------+ +----------+
 |                    | |                    | |                    |
 +--------------------+ +--------------------+ +--------------------+
 |       node-A       | |       node-B       | |       node-C       |
 +--------------------+ +--------------------+ +--------------------+
```



## keepalived and haproxy 

当设置具有多个控制平面的集群时，可以通过将 API Server 实例放在负载均衡器后面，并在为新集群使用而运行 `kubeadm init` 时使用` -- control-plane-endpoint `选项来实现更高的可用性。

### keepalived 配置

Keepalived 配置由两个文件组成: 服务配置文件和一个健康检查脚本，该脚本将定期调用，以验证持有虚拟 IP 的节点是否仍在运行。

假定这些文件位于/etc/keepalived 目录中。下面的配置已经成功地用于 keepalived 版本2.0.17:

```bash
! /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
}
vrrp_script check_apiserver {
  script "/etc/keepalived/check_apiserver.sh"
  interval 3
  weight -2
  fall 10
  rise 2
}

vrrp_instance VI_1 {
    state ${STATE}
    interface ${INTERFACE}
    virtual_router_id ${ROUTER_ID}
    priority ${PRIORITY}
    authentication {
        auth_type PASS
        auth_pass ${AUTH_PASS}
    }
    virtual_ipaddress {
        ${APISERVER_VIP}
    }
    track_script {
        check_apiserver
    }
}
```

有一些`bash`可变样式的占位符需要填写：

- `${STATE}`是`MASTER`一个和`BACKUP`为所有其他主机，因此虚拟IP最初将被分配到`MASTER`。
- `${INTERFACE}`是参与虚拟IP协商的网络接口，例如`eth0`。
- `${ROUTER_ID}`对于所有`keepalived`集群主机应该是相同的，而在同一子网中的所有集群中应该是唯一的。许多发行版将其值预先配置为`51`.
- `${PRIORITY}`在控制平面节点上应该比在备份上更高。因此`101`和`100`分别就足够了。
- `${AUTH_PASS}`所有`keepalived`集群主机都应该相同，例如`42`
- `${APISERVER_VIP}`是`keepalived`集群主机之间协商的虚拟 IP 地址。

### 健康检查脚本

`/etc/keepalived/check_apiserver.sh`

```bash
#!/bin/sh

errorExit() {
    echo "*** $*" 1>&2
    exit 1
}

curl --silent --max-time 2 --insecure https://localhost:${APISERVER_DEST_PORT}/ -o /dev/null || errorExit "Error GET https://localhost:${APISERVER_DEST_PORT}/"
if ip addr | grep -q ${APISERVER_VIP}; then
    curl --silent --max-time 2 --insecure https://${APISERVER_VIP}:${APISERVER_DEST_PORT}/ -o /dev/null || errorExit "Error GET https://${APISERVER_VIP}:${APISERVER_DEST_PORT}/"
fi

```

有一些`bash`可变样式的占位符需要填写：

- `${APISERVER_VIP}`是`keepalived`集群主机之间协商的虚拟 IP 地址。
- `${APISERVER_DEST_PORT}` Kubernetes 与 API Server 通信的端口。

## Haproxy

Haproxy 配置由一个文件组成: 假定驻留在/etc/haproxy 目录中的服务配置文件。下面的配置已经成功地用于 haproxy 2.1.4版本:

```
# /etc/haproxy/haproxy.cfg
#---------------------------------------------------------------------
# Global settings
#---------------------------------------------------------------------
global
    log /dev/log local0
    log /dev/log local1 notice
    daemon

#---------------------------------------------------------------------
# common defaults that all the 'listen' and 'backend' sections will
# use if not designated in their block
#---------------------------------------------------------------------
defaults
    mode                    http
    log                     global
    option                  httplog
    option                  dontlognull
    option http-server-close
    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 1
    timeout http-request    10s
    timeout queue           20s
    timeout connect         5s
    timeout client          20s
    timeout server          20s
    timeout http-keep-alive 10s
    timeout check           10s

#---------------------------------------------------------------------
# apiserver frontend which proxys to the control plane nodes
#---------------------------------------------------------------------
frontend apiserver
    bind *:${APISERVER_DEST_PORT}
    mode tcp
    option tcplog
    default_backend apiserver

#---------------------------------------------------------------------
# round robin balancing for apiserver
#---------------------------------------------------------------------
backend apiserver
    option httpchk GET /healthz
    http-check expect status 200
    mode tcp
    option ssl-hello-chk
    balance     roundrobin
        server ${HOST1_ID} ${HOST1_ADDRESS}:${APISERVER_SRC_PORT} check
        # [...]
```

同样，有一些`bash`可变样式的占位符可以扩展：

- `${APISERVER_DEST_PORT}` Kubernetes 与 API Server 通信的端口。
- `${APISERVER_SRC_PORT}` API Server 实例使用的端口
- `${HOST1_ID}` 第一个负载平衡的 API 服务器主机的符号名称
- `${HOST1_ADDRESS}` 第一个负载平衡的 API Server 主机的可解析地址（DNS 名称、IP 地址）
- 额外的`server`行，每个负载平衡的 API Server 主机一个

### 选项 1：在操作系统上运行服务

为了在操作系统上运行这两个服务，可以使用相应发行版的包管理器来安装软件。如果它们将在不属于 Kubernetes 集群的专用主机上运行，这可能是有意义的。

现在已经安装了上述配置，可以启用和启动服务。在最近的基于 RedHat 的系统上，`systemd`将用于此：

```
# systemctl enable haproxy --now
# systemctl enable keepalived --now
```

### 选项 2：将服务作为静态 Pod 运行

如果`keepalived`并且`haproxy`将在控制平面节点上运行，它们可以配置为作为静态 Pod 运行。这里所需要的只是`/etc/kubernetes/manifests`在引导集群之前将各自的清单文件放在目录中。在引导过程中，`kubelet`将启动进程，以便集群在启动时可以使用它们。这是一个优雅的解决方案，特别是在[Stacked control plane 和 etcd nodes](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/#stacked-control-plane-and-etcd-nodes)下描述的设置。

对于此设置，需要在`/etc/kubernetes/manifests`（首先创建目录）中创建两个清单文件。

对于清单`keepalived`，`/etc/kubernetes/manifests/keepalived.yaml`：

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  name: keepalived
  namespace: kube-system
spec:
  containers:
  - image: osixia/keepalived:2.0.17
    name: keepalived
    resources: {}
    securityContext:
      capabilities:
        add:
        - NET_ADMIN
        - NET_BROADCAST
        - NET_RAW
    volumeMounts:
    - mountPath: /usr/local/etc/keepalived/keepalived.conf
      name: config
    - mountPath: /etc/keepalived/check_apiserver.sh
      name: check
  hostNetwork: true
  volumes:
  - hostPath:
      path: /etc/keepalived/keepalived.conf
    name: config
  - hostPath:
      path: /etc/keepalived/check_apiserver.sh
    name: check
status: {}
```

对于清单`haproxy`，`/etc/kubernetes/manifests/haproxy.yaml`：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: haproxy
  namespace: kube-system
spec:
  containers:
  - image: haproxy:2.1.4
    name: haproxy
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: localhost
        path: /healthz
        port: ${APISERVER_DEST_PORT}
        scheme: HTTPS
    volumeMounts:
    - mountPath: /usr/local/etc/haproxy/haproxy.cfg
      name: haproxyconf
      readOnly: true
  hostNetwork: true
  volumes:
  - hostPath:
      path: /etc/haproxy/haproxy.cfg
      type: FileOrCreate
    name: haproxyconf
status: {}
```

请注意，这里再次需要填充一个占位符：`${APISERVER_DEST_PORT}`需要保持与 in 相同的值`/etc/haproxy/haproxy.cfg`（见上文）。

此组合已成功用于示例中使用的版本。其他版本也可能工作，或者可能需要更改配置文件。

服务启动后，现在可以使用 Kubernetes 集群进行引导`kubeadm init`（见[下文](https://github.com/kubernetes/kubeadm/blob/master/docs/ha-considerations.md#bootstrap-the-cluster)）。

## kube-vip 

https://blog.csdn.net/u013812710/article/details/116451771

与传统的 keepalived 和 haproxy 方法相比，kube-vip 在一个服务中实现了虚拟 IP 和负载平衡的管理。与上面的选项2类似，kube-vip 将作为控制平面节点上的静态 pod 运行。

就像 keepalived 一样，协商虚拟 IP 的主机需要在同一个 IP 子网中。与 haproxy 类似，基于流的负载平衡允许 TLS 终止由其后面的 API Server 实例处理。

配置文件`/etc/kube-vip/config.yaml`如下所示：

```yaml
localPeer:
  id: ${ID}
  address: ${IPADDR}
  port: 10000
remotePeers:
- id: ${PEER1_ID}
  address: ${PEER1_IPADDR}
  port: 10000
# [...]
vip: ${APISERVER_VIP}
gratuitousARP: true
singleNode: false
startAsLeader: ${IS_LEADER}
interface: ${INTERFACE}
loadBalancers:
- name: API Server Load Balancer
  type: tcp
  port: ${APISERVER_DEST_PORT}
  bindToVip: false
  backends:
  - port: ${APISERVER_SRC_PORT}
    address: ${HOST1_ADDRESS}
  # [...]
```

`bash`要扩展的样式占位符如下：

- `${ID}` 当前主机的符号名称
- `${IPADDR}` 当前主机的 IP 地址
- `${PEER1_ID}` 第一个 vIP 对等点的符号名称
- `${PEER1_IPADDR}` 第一个 vIP 对等体的 IP 地址
- 其他 vIP 对等点的条目 ( `id`, `address`, `port`) 可以跟随
- `${APISERVER_VIP}`是`kube-vip`集群主机之间协商的虚拟 IP 地址。
- `${IS_LEADER}`只`true`针对一位领导者，`false`其他人
- `${INTERFACE}`是参与虚拟IP协商的网络接口，例如`eth0`。
- `${APISERVER_DEST_PORT}` Kubernetes 与 API Server 通信的端口。
- `${APISERVER_SRC_PORT}` API Server 实例使用的端口
- `${HOST1_ADDRESS}` 第一个负载均衡的 API Server 主机的 IP 地址
- 其他负载均衡 API 服务器主机的条目 ( `port`, `address`) 可以遵循

要使用集群启动服务，现在`kube-vip.yaml`需要将清单放入`/etc/kubernetes/manifests`（首先创建目录）。它可以使用`kube-vip`docker 镜像生成：

```
# docker run -it --rm plndr/kube-vip:0.1.1 /kube-vip sample manifest \
    | sed "s|plndr/kube-vip:'|plndr/kube-vip:0.1.1'|" \
    | sudo tee /etc/kubernetes/manifests/kube-vip.yaml
```

结果,/etc/kubernetes/manifest/kube-vip. yaml，看起来像这样:

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  name: kube-vip
  namespace: kube-system
spec:
  containers:
  - command:
    - /kube-vip
    - start
    - -c
    - /vip.yaml
    image: 'plndr/kube-vip:0.1.1'
    name: kube-vip
    resources: {}
    securityContext:
      capabilities:
        add:
        - NET_ADMIN
        - SYS_TIME
    volumeMounts:
    - mountPath: /vip.yaml
      name: config
  hostNetwork: true
  volumes:
  - hostPath:
      path: /etc/kube-vip/config.yaml
    name: config
status: {}
```

通过启动服务，现在可以使用 kubeadm init 引导 Kubernetes 集群(见下文)。

## 引导集群

现在可以进行[使用 kubeadm 创建高可用性集群中](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/)所述的实际集群引导程序。

请注意，如果`${APISERVER_DEST_PORT}`已配置为与`6443`上述配置不同的值，则`kubeadm init`需要告知 API Server 使用该端口。假设在一个新的集群端口 8443 用于负载均衡的 API Server 和一个带有 DNS name 的虚拟 IP，则需要传递`vip.mycluster.local`一个参数，如下所示：`--control-plane-endpoint``kubeadm`

```
# kubeadm init --control-plane-endpoint vip.mycluster.local:8443 [additional arguments ...]
```
