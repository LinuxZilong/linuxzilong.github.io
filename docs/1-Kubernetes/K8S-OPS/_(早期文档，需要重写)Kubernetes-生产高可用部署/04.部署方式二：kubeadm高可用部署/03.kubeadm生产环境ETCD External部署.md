---
title: 03.kubeadm生产环境ETCD External部署
---

也可以使用 kubeadm 创建一个外部高可用 etcd 集群

![外部 etcd 拓扑](https://d33wubrfki0l68.cloudfront.net/ad49fffce42d5a35ae0d0cc1186b97209d86b99c/5a6ae/images/kubeadm/kubeadm-ha-topology-external-etcd.svg)

https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/

## 安装docker kubelet kubectl kubeadm 配置cgroup

```bash
mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": [
      "https://registry.docker-cn.com",
      "https://docker.mirrors.ustc.edu.cn"
    ],
  "exec-opts": ["native.cgroupdriver=systemd"]
}
EOF
systemctl daemon-reload && systemctl enable docker && systemctl restart docker
```

## 列出可用镜像

```bash
kubeadm config images list --kubernetes-version 1.20.5  --image-repository registry.aliyuncs.com/google_containers
#预下载
docker pull registry.aliyuncs.com/google_containers/etcd:3.4.13-0
```

## 将 kubelet 配置为 etcd 的服务管理器。

> **说明：** 你必须在要运行 etcd 的所有主机上执行此操作。

由于 etcd 是首先创建的，因此你必须通过创建具有更高优先级的新文件来覆盖 kubeadm 提供的 kubelet 单元文件。

```bash
mkdir -p /etc/systemd/system/kubelet.service.d/
cat << EOF > /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
# 注意pause版本
[Service]
ExecStart=
# 将下面的 "systemd" 替换为你的容器运行时所使用的 cgroup 驱动。
# kubelet 的默认值为 "cgroupfs"。
ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --cgroup-driver=systemd --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.2
Restart=always
EOF

systemctl daemon-reload
systemctl restart kubelet
```

检查 kubelet 的状态以确保其处于运行状态：

```shell
systemctl status kubelet
```

## 为 kubeadm 创建配置文件。

使用以下脚本为每个将要运行 etcd 成员的主机生成一个 kubeadm 配置文件。

> apiVersion: "kubeadm.k8s.io/v1beta3"
>
> 早些的版本应使用apiVersion: "kubeadm.k8s.io/v1beta2"，如1.20.0

```sh
# 使用 IP 或可解析的主机名替换 HOST0、HOST1 和 HOST2
export HOST0=10.0.0.10
export HOST1=10.0.0.20
export HOST2=10.0.0.30

# 创建临时目录来存储将被分发到其它主机上的文件
mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/

ETCDHOSTS=(${HOST0} ${HOST1} ${HOST2})
NAMES=("etcd-0" "etcd-1" "etcd-2")

for i in "${!ETCDHOSTS[@]}"; do
HOST=${ETCDHOSTS[$i]}
NAME=${NAMES[$i]}
cat << EOF > /tmp/${HOST}/kubeadmcfg.yaml
apiVersion: "kubeadm.k8s.io/v1beta2"
kind: ClusterConfiguration
etcd:
    local:
        serverCertSANs:
        - "${HOST}"
        peerCertSANs:
        - "${HOST}"
        extraArgs:
            initial-cluster: ${NAMES[0]}=https://${ETCDHOSTS[0]}:2380,${NAMES[1]}=https://${ETCDHOSTS[1]}:2380,${NAMES[2]}=https://${ETCDHOSTS[2]}:2380
            initial-cluster-state: new
            name: ${NAME}
            listen-peer-urls: https://${HOST}:2380
            listen-client-urls: https://${HOST}:2379
            advertise-client-urls: https://${HOST}:2379
            initial-advertise-peer-urls: https://${HOST}:2380
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
kubernetesVersion: v1.20.0            
EOF
done
```

## 生成证书颁发机构

如果你已经拥有 CA，那么唯一的操作是复制 CA 的 `crt` 和 `key` 文件到 `etc/kubernetes/pki/etcd/ca.crt` 和 `/etc/kubernetes/pki/etcd/ca.key`。 复制完这些文件后继续下一步，“为每个成员创建证书”。

如果你还没有 CA，则在 `$HOST0`（你为 kubeadm 生成配置文件的位置）上运行此命令。

```
kubeadm init phase certs etcd-ca
```

这一操作创建如下两个文件

- `/etc/kubernetes/pki/etcd/ca.crt`
- `/etc/kubernetes/pki/etcd/ca.key`

## 为每个成员创建证书

```shell
kubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/${HOST2}/
# 清理不可重复使用的证书
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm init phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/${HOST1}/
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml
# 不需要移动 certs 因为它们是给 HOST0 使用的

# 清理不应从此主机复制的证书
find /tmp/${HOST2} -name ca.key -type f -delete
find /tmp/${HOST1} -name ca.key -type f -delete
```

## 分发证书和 kubeadm 配置

证书已生成，现在必须将它们移动到对应的主机。

```shell
USER=k8s
HOST=${HOST1}
scp -r /tmp/${HOST}/* ${USER}@${HOST}:
ssh ${USER}@${HOST}
USER@HOST $ sudo -Es
root@HOST $ chown -R root:root pki
root@HOST $ mv pki /etc/kubernetes/
######或者

rsync -a /tmp/${HOST2}/pki ${HOST2}:/etc/kubernetes/
rsync -a /tmp/${HOST1}/pki ${HOST1}:/etc/kubernetes/

rsync -a /tmp/${HOST2}/kubeadmcfg.yaml ${HOST2}:~/.
rsync -a /tmp/${HOST1}/kubeadmcfg.yaml ${HOST1}:~/.
rsync -a /tmp/${HOST0}/kubeadmcfg.yaml ${HOST0}:~/.

##
USER=k8s
ETCD_HOSTS=(10.0.0.10 10.0.0.20 10.0.0.30)
for ip in ${ETCD_HOSTS[@]}; do
    scp -r /tmp/${ip}/* ${USER}@${ip}:
    ssh ${USER}@${ip}  /bin/bash << EOF
	sudo -Es
	chown -R root:root pki
	mv pki /etc/kubernetes/
EOF
done
```

## 部署ETCD节点

```
kubeadm init phase etcd local --config=./kubeadmcfg.yaml
```

## 可选：检查群集运行状况

```shell
docker run --rm -it \
--net host \
-v /etc/kubernetes:/etc/kubernetes registry.aliyuncs.com/google_containers/etcd:3.4.13-0 etcdctl \
--cert /etc/kubernetes/pki/etcd/peer.crt \
--key /etc/kubernetes/pki/etcd/peer.key \
--cacert /etc/kubernetes/pki/etcd/ca.crt \
--endpoints https://10.0.0.30:2379 \
endpoint health  -w table --cluster

+------------------------+--------+-------------+-------+
|        ENDPOINT        | HEALTH |    TOOK     | ERROR |
+------------------------+--------+-------------+-------+
| https://10.0.0.10:2379 |   true | 13.224338ms |       |
| https://10.0.0.30:2379 |   true | 14.218999ms |       |
| https://10.0.0.20:2379 |   true | 15.903513ms |       |
+------------------------+--------+-------------+-------+
```

- 将 `${ETCD_TAG}` 设置为你的 etcd 镜像的版本标签，例如 `3.4.3-0`。 要查看 kubeadm 使用的 etcd 镜像和标签，请执行 `kubeadm config images list --kubernetes-version ${K8S_VERSION}`， 例如，其中的 `${K8S_VERSION}` 可以是 `v1.20.0`。
- 将 `${HOST0}` 设置为要测试的主机的 IP 地址。

## 接下来 

一旦拥有了一个正常工作的 3 成员的 etcd 集群，你就可以基于 [使用 kubeadm 外部 etcd 的方法](https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/high-availability/)， 继续部署一个高可用的控制平面。

### 复制证书到控制平面

```shell
export CONTROL_PLANE="root@10.0.0.10"
scp /etc/kubernetes/pki/etcd/ca.crt "${CONTROL_PLANE}":
scp /etc/kubernetes/pki/apiserver-etcd-client.crt "${CONTROL_PLANE}":
scp /etc/kubernetes/pki/apiserver-etcd-client.key "${CONTROL_PLANE}":
```

### 设置第一个控制平面节点

用以下内容创建一个名为 `kubeadm-config.yaml` 的文件：

```yaml
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: stable
controlPlaneEndpoint: "LOAD_BALANCER_DNS:LOAD_BALANCER_PORT"
etcd:
    external:
        endpoints:
        - https://ETCD_0_IP:2379
        - https://ETCD_1_IP:2379
        - https://ETCD_2_IP:2379
        caFile: /etc/kubernetes/pki/etcd/ca.crt
        certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
        keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key
```

>这里的内部（stacked） etcd 和外部 etcd 之前的区别在于设置外部 etcd
>需要一个 `etcd` 的 `external` 对象下带有 etcd 端点的配置文件。
>如果是内部 etcd，是自动管理的。
>
>- 在你的集群中，将配置模板中的以下变量替换为适当值：
> - `LOAD_BALANCER_DNS`
> - `LOAD_BALANCER_PORT`
> - `ETCD_0_IP`
> - `ETCD_1_IP`
> - `ETCD_2_IP`

以下的步骤与设置内置 etcd 的集群是相似的：

1. 在节点上运行 `sudo kubeadm init --config kubeadm-config.yaml --upload-certs` 命令。
2. 记下输出的 join 命令，这些命令将在以后使用。
3. 应用你选择的 CNI 插件。
